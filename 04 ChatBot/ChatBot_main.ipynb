{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Project Goal \n","\n","Разработаем чатбот-барохольщик. На входе он принимает любой запрос от пользователя, затем классифицирует его на продуктовый или другой тип запроса.\n","Если запрос продуктовый, то на выходе пользователь получит id товара, который подходит под его запрос, если запрос другой - то бот перенаправит вопрос в болталку и даст ответ на вопрос, который был ближе всего к вопросу, который был взят из корпуса текста с форумов на mail.ru.\n","\n","В процессе проекта будем использовать векторизацию текста, модель Word2Vec, логистическую регрессию."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /home/nlp/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import os\n","import re\n","import pickle\n","import string\n","import annoy\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, RegexpTokenizer\n","nltk.download('stopwords')\n","\n","from gensim.models import Word2Vec\n","from joblib import dump, load\n","from pymorphy2 import MorphAnalyzer\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from string import punctuation\n","\n","import numpy as np\n","import tqdm\n","from tqdm import tqdm_notebook\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Classifier\n","Обучим классификатор - будем разделять вопросы от пользователя на продуктовые запросы и все остальные.\n","\n","Предобработаем ответы mail.ru из файла: к каждому вопросу присоединим 1 ответ и запишем в файл на будущее. Это позволит нам сэкономить время и ресурсы при дальнейшем препроцессинге текста."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_6758/1279344030.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for line in tqdm_notebook(fin):\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b02b73160ed4b6f99b2ff45421ec698","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["question = None\n","written = False\n","\n","'''Идем по всем записям, берем первую строку как вопрос и после знака --- находим ответ'''\n","with open('prepared_answers.txt', 'w') as fout:\n","    with open('answers.txt', 'r') as fin:\n","        for line in tqdm_notebook(fin):\n","            if line.startswith('---'):\n","                written = False\n","                continue\n","            if not written and question is not None:\n","                fout.write(question.replace('\\t', ' ').strip() + '\\t' + line.replace('\\t', ' '))\n","                written = True\n","                question = None\n","                continue\n","            if not written:\n","                question = line.strip()\n","                continue"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["'''Функция для нормализации текста - приведение к нижнему регистру, удаление стоп-слов, знаков пунктуации, латинских букв, лемматизация'''\n","def preprocess_text(txt: str, sw = []) -> str:\n","    exclude = list(punctuation)\n","    '''Стоп-слова из библиотеки nltk'''\n","    sw_1 = list(stopwords.words('russian'))\n","    morpher = MorphAnalyzer()\n","    txt = str(txt)\n","    '''Делим предложение на слова по знакам препинания, приводим все слова к нижнему регистру, удаляем знаки препинания, стоп-слова и цифры'''\n","    tokenizer = RegexpTokenizer(\"\\w+|[^\\w\\s]+\")\n","    txt = [i.lower() for i in tokenizer.tokenize(txt) if i not in exclude and i not in sw and i not in sw_1 and len(i) > 2 and \\\n","          re.search('(\\d+)|\\)|\\(|\\.|\\:|\\;|\\!|\\?|\\<|\\>|[A-Za-z]', i) is None]\n","    '''Делаем лемматизацию'''\n","    txt = [morpher.parse(word)[0].normal_form for word in txt]\n","    return \" \".join(txt)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["'''Дополнительные стоп-слова'''\n","sw = ['всё', 'тому', 'представьте', 'чьё', 'смысле', 'статься', 'знаешь', 'получается', 'или', 'выяснилось', 'удивительно',\n","      'которых', 'оти', 'значится', 'сказано', 'никогда', 'об', 'образом', 'моему', 'самом', 'свыше', 'во-вторых', 'как', 'чья', 'иметь', \n","      'следовательно', 'серьезно', 'точки', 'точнее', 'поди', 'бы', 'но', 'само', 'осле', 'конено', 'около', 'из-за', 'отому', 'скажем',\n","      'новость', 'достаточно', 'издревле', 'когда', 'любом', 'ваша', 'мол', 'конец', 'словно', 'никто', 'уж', 'да', 'актуальный', 'попросту',\n","      'бесспорно', 'долженствовать', 'было', 'которая', 'хорошо', 'случайно', 'хоть', 'ко', 'видать', 'повседневной', 'этот', 'позволь',\n","      'наверно', 'прочему', 'самым', 'буквально', 'здесь' 'воистину', 'право', 'ничего', 'понятно', 'разнообразный', 'вообразите', 'июль',\n","      'эта', 'тогда', 'дальше', 'конечно', 'наоборот', 'бывает', 'твоя', 'мне', 'полагать', 'важно', 'нам', 'представленный', 'проще',\n","      'очередной', 'выясняется', 'странно', 'устоявшееся', 'моя', 'всевозможный', 'долженствующий', 'прочий', 'где', 'откровенно', 'принято',\n","      'беспрецедентный', 'целом', 'один', 'нашей', 'абсолютно', 'следствие', 'также', 'они', 'после', 'согласиться', 'реальный', 'по-твоему',\n","      'имеется', 'лет', 'небось', 'казалось', 'своему', 'скорее', 'неизгладимый', 'напротив', 'март', 'наверняка', 'обычно', 'нынче',\n","      'моей', 'примечательно', 'против', 'наконец', 'так', 'тыс', 'какой', 'много', 'ней', 'если', 'должно', 'ваш', 'нее', 'быть', 'этом',\n","      'чем', 'возможно', 'вестимо', 'знать', 'вроем', 'выражаясь', 'одним', 'теерь', 'кажется', 'многие', 'от', 'есть', 'себе', 'просто',\n","      'всяком', 'даже', 'ничто', 'обычаю', 'знает', 'исходя', 'несколько', 'предельно', 'ли', 'она', 'ерез', 'вас', 'ведь', 'имеются', 'более',\n","      'он', 'кто', 'по', 'уть', 'свою' 'твоему', 'них', 'кстати', 'од', 'крайне', 'ри', 'говори', 'примеру', 'их', 'вашему', 'всякого',\n","      'исключение', 'через', 'того', 'год', 'оказывается', 'по-ихнему', 'вишь', 'конце', 'вашего', 'тоб', 'кб', 'пожалуйста', 'сейас',\n","      'вам', 'эй', 'над', 'естественно', 'без', 'нибудь', 'нечего', 'еред', 'куда', 'вообще-то', 'менее', 'по-вашему', 'хм', 'что', 'про',\n","      'всей', 'им', 'наша', 'кажись', 'твоего', 'своей', 'наверное', 'собой', 'нами', 'предположительно', 'мы', 'всех', 'по-моему', 'исстари',\n","      'себя', 'некто', 'иногда', 'совершенно', 'для', 'вы', 'этих', 'как-то', 'разумеется', 'ему', 'всегда', 'чего', 'другие', 'во-первых',\n","      'позвольте', 'вип', 'очевидно', 'так-то', 'замыслу', 'нарочно', 'короче', 'то', 'помилуйте', 'сути', 'по-хорошему', 'иначе', 'нередко',\n","      'чтобы', 'всего', 'все', 'надо', 'весь', 'раз', 'известно', 'из', 'свой', 'по-видимому', 'тут', 'под', 'мой', 'ул', 'луше', 'ей', 'только',\n","      'вероятно', 'видимо', 'руб', 'слову', 'нет', 'за', 'тем', 'во', 'например', 'помимо', 'такой', 'будет', 'помилуй', 'тоже', 'скажут',\n","      'видно', 'пожалуй', 'общеизвестно', 'чье', 'вдобавок', 'впрочем', 'соответственно', 'зачастую', 'сам', 'том', 'там', 'полагается',\n","      'меня', 'однако', 'слышь', 'итак', 'подобное', 'действительно', 'еще', 'его', 'наше', 'данный', 'твой', 'ай', 'необходимо', 'три',\n","      'далее', 'тот', 'эм', 'же', 'жаль', 'другими', 'видишь', 'некоторых', 'прежде', 'который', 'допустим', 'положено', 'по-нашему', 'весьма',\n","      'не', 'ро', 'той', 'этого', 'эти', 'напомним', 'правда', 'выходит', 'перед', 'определенно', 'какая', 'относительно', 'сразу', 'имя',\n","      'чей', 'ее', 'отом', 'неё', 'это', 'два', 'эту', 'разве', 'вне', 'тобы', 'знамо', 'всем', 'вернее', 'будто', 'вновь', 'которые', 'данным',\n","      'кроме', 'таким', 'нечто', 'кого', 'больше', 'ом', 'уже', 'прочего', 'очень', 'при', 'те', 'вообще', 'ясно', 'ну', 'октябрь', 'до',\n","      'на', 'в-третьих', 'ежели', 'общем', 'ещё', 'похоже', 'ибо', 'оно', 'ты', 'сверх', 'значит', 'безусловно', 'правильнее', 'ниего', 'него', 'совсем',\n","      'вами', 'общем-то', 'всю', 'наш', 'всему', 'всякий', 'ой', 'этой', 'ни', 'со', 'вот', 'день', 'может', 'мб', 'тебя', 'нас',\n","      'оять', 'хотя', 'прочим', 'этим', 'между', 'ним', 'вероятнее', 'бывало', 'примерно', 'вдруг', 'либо']"]},{"cell_type":"markdown","metadata":{},"source":["## 1.1 Product Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["df = pd.read_csv('ProductsDataset.csv')\n","df.rename(columns = {'descrirption' : 'description'}, inplace = True) \n","df = df.fillna('0')\n","df['description_full'] = df['title'] + ' ' + df['description']\n","df = df.loc[df['description_full'] != '00']"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["'''Нормализуем полные описания товаров (название товара + его описание)'''\n","df['description_full_norm'] = df.apply(lambda x: preprocess_text(x['description_full'], sw), axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Мини-датасет продукт - таргет 1'''\n","df_product = pd.DataFrame()\n","df_product['description'] = df['description_full_norm']\n","df_product['target'] = 1\n","df_product.loc[df_product['description'] != 'nan']"]},{"cell_type":"markdown","metadata":{},"source":["## 1.2 Chat Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_802088/3231642506.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for line in tqdm_notebook(fin):\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4bcb00611bb3427abf5d2194a56f7475","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["'''Отбираем из корпуса со всеми ответами записи, по количеству = количеству продуктовых записей\n","Это необходимо для сбалансированной выборки для обучения будущей модели'''\n","prepared_sentences = []\n","c = 0\n","\n","with open('prepared_answers.txt', 'r') as fin:\n","    for line in tqdm_notebook(fin):\n","        spls = preprocess_text(line, sw)\n","        if spls:\n","            prepared_sentences.append(spls)\n","            c += 1\n","            if c >= len(df_product):\n","                break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Мини-датасет вопрос - таргет 0'''\n","df_chat = pd.DataFrame()\n","df_chat['description'] = prepared_sentences\n","df_chat['target'] = 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Объединяем продуктовый и чат датасеты в один - это наша выборка'''\n","main_df = pd.concat([df_product, df_chat], ignore_index=True)"]},{"cell_type":"markdown","metadata":{},"source":["## 1.3 ML"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vectorizer = TfidfVectorizer()\n","\n","X = main_df['description']\n","y = main_df['target']\n","\n","X = vectorizer.fit_transform(X)\n","y = np.array(y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9826713176550017\n"]}],"source":["log_reg = LogisticRegression(random_state=0).fit(X_train, y_train)\n","y_pred = log_reg.predict(X_test)\n","\n","accuracy = (y_pred == y_test).mean()\n","print(accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["accuracy - 0.98 (!) - это отличный результат. Берем логистическую регрессию в качестве основной модели для нашего классификатора."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["array([1, 0])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["'''Визуализируем работу классификатора'''\n","vec = vectorizer.transform(['толстовка', 'город'])\n","log_reg.predict(vec)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Сохраняем модель'''\n","pickle.dump(log_reg, open(f'models/log_reg.sav', 'wb'))"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Chat Branch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_4614/4146970320.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for line in tqdm_notebook(fin):\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09e23c0aff114e189ceb55416170fb0a","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["'''Нормализуем ответы из корпуса с вопросами'''\n","sentences = []\n","c = 0\n","\n","with open('answers.txt', 'r') as fin:\n","    for line in tqdm_notebook(fin):\n","        spls = preprocess_text(line, sw)\n","        sentences.append(spls)\n","        c += 1\n","        if c > 500000:\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Обучим модель word2vec на наших вопросах'''\n","sentences = [i for i in sentences if len(i) > 2]\n","answer_model = Word2Vec(sentences=sentences, vector_size=100, min_count=1, window=5)\n","pickle.dump(answer_model, open(f'models/answer_model_w2v.sav', 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_4614/2233210317.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for line in tqdm_notebook(f):\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc2c635652dd49f0b2a4771db1b2ff3c","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["True"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["'''Сложим в индекс все вопросы. Используем библиотеку annoy. \n","Проходимся по всем ответам, считаем, что вектор предложения - сумма word2vecов слов (усредненная), которые входят в него'''\n","\n","answer_index = annoy.AnnoyIndex(100 ,'angular')\n","\n","answer_index_map = {}\n","counter = 0\n","\n","with open('prepared_answers.txt', 'r') as f:\n","    for line in tqdm_notebook(f):\n","        try:\n","            n_w2v = 0\n","            spls = line.split('\\t')\n","            answer_index_map[counter] = spls[1]\n","            question = preprocess_text(spls[0], sw)\n","            vector = np.zeros(100)\n","            for word in question:\n","                if word in answer_model.wv:\n","                    vector += answer_model.wv[word]\n","                    n_w2v += 1\n","            if n_w2v > 0:\n","                vector = vector / n_w2v\n","            answer_index.add_item(counter, vector)\n","\n","            counter += 1\n","\n","            if counter > 500000:\n","                break\n","            \n","        except:\n","            print(spls)\n","            pass\n","\n","answer_index.build(10)\n","answer_index.save('models/answer.ann')\n","pickle.dump(answer_index_map, open(f'models/answer_index_map.pkl', 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''На входе получаем вопрос от пользователя, который был классифицирован, как другой (не продуктовый)\n","Выводим тот ответ на вопрос, который ближе всего по смыслу к вопросу пользователя'''\n","def find_answer(question, answer_model, answer_index, answer_index_map):\n","    preprocessed_question = preprocess_text(question, sw)\n","    n_w2v = 0\n","    vector = np.zeros(100)\n","    for word in preprocessed_question:\n","        if word in answer_model.wv:\n","            vector += answer_model.wv[word]\n","            n_w2v += 1\n","    if n_w2v > 0:\n","        vector = vector / n_w2v\n","    answer_index_1 = answer_index.get_nns_by_vector(vector, 1)\n","    return answer_index_map[answer_index_1[0]]"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Product Branch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['title_norm'] = df.apply(lambda x: preprocess_text(x['title'], sw), axis=1)\n","\n","product_list = df['title_norm']\n","id_list = df['product_id']\n","\n","product_model = Word2Vec(sentences=product_list, vector_size=100, min_count=1, window=5)\n","pickle.dump(answer_model, open(f'models/product_model_w2v.sav', 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["product_index = annoy.AnnoyIndex(100 ,'angular')\n","\n","product_index_map = {}\n","counter = 0\n","\n","for t, product in enumerate(product_list):\n","    n_w2v = 0\n","    product_index_map[counter] = spls[1]\n","    vector = np.zeros(100)\n","    for word in product:\n","        if word in product_model.wv:\n","            vector += product_model.wv[word]\n","            n_w2v += 1\n","    if n_w2v > 0:\n","        vector = vector / n_w2v\n","    product_index.add_item(counter, vector)\n","\n","    counter += 1\n","    \n","product_index.build(10)\n","product_index.save('models/product.ann')\n","pickle.dump(product_index_map, open(f'models/product_index_map.pkl', 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''На входе получаем вопрос от пользователя, который был классифицирован, как продуктовый.\n","Выводим тот id товара, чье название оказалось ближе всего к запросу пользователя'''\n","def find_product(question, product_model, product_index, product_index_map):\n","    preprocessed_question = preprocess_text(question, sw)\n","    n_w2v = 0\n","    vector = np.zeros(100)\n","    for word in preprocessed_question:\n","        if word in product_model.wv:\n","            vector += product_model.wv[word]\n","            n_w2v += 1\n","    if n_w2v > 0:\n","        vector = vector / n_w2v\n","    product_index_1 = product_index.get_nns_by_vector(vector, 1)\n","    return product_index_map[product_index_1[0]]"]},{"cell_type":"markdown","metadata":{},"source":["# Main algoritm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_answer(question, log_reg, answer_model, answer_index, answer_index_map, product_model, product_index, product_index_map):\n","    question = preprocess_text(question, sw)\n","    vec = vectorizer.transform([question])\n","    clf_result = log_reg.predict(vec).tolist()[0]\n","    \n","    if clf_result == 0:\n","        result = find_answer(question, answer_model, answer_index, answer_index_map)\n","    elif clf_result == 1:\n","        result = find_product(question, product_model, product_index, product_index_map)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_answer('Юбка детская ORBY', log_reg, answer_model, answer_index, answer_index_map, product_model, product_index, product_index_map)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ChatBotPractice.ipynb","provenance":[]},"interpreter":{"hash":"939f9d531ff333af0130dd8765ac0b166b30ee19b2f98cd50da4da1de41df6cc"},"kernelspec":{"display_name":"Python 3.9.7 ('vtb')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
